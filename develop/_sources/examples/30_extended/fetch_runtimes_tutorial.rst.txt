
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/30_extended/fetch_runtimes_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_30_extended_fetch_runtimes_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_30_extended_fetch_runtimes_tutorial.py:


==========================================
Measuring runtimes for Scikit-learn models
==========================================

The runtime of machine learning models on specific datasets can be a deciding
factor on the choice of algorithms, especially for benchmarking and comparison
purposes. OpenML's scikit-learn extension provides runtime data from runs of
model fit and prediction on tasks or datasets, for both the CPU-clock as well
as the actual wallclock-time incurred. The objective of this example is to
illustrate how to retrieve such timing measures, and also offer some potential
means of usage and interpretation of the same.

It should be noted that there are multiple levels at which parallelism can occur.

* At the outermost level, OpenML tasks contain fixed data splits, on which the
  defined model/flow is executed. Thus, a model can be fit on each OpenML dataset fold
  in parallel using the `n_jobs` parameter to `run_model_on_task` or `run_flow_on_task`
  (illustrated under Case 2 & 3 below).

* The model/flow specified can also include scikit-learn models that perform their own
  parallelization. For instance, by specifying `n_jobs` in a Random Forest model definition
  (covered under Case 2 below).

* The sklearn model can further be an HPO estimator and contain it's own parallelization.
  If the base estimator used also supports `parallelization`, then there's at least a 2-level nested
  definition for parallelization possible (covered under Case 3 below).

We shall cover these 5 representative scenarios for:

* (Case 1) Retrieving runtimes for Random Forest training and prediction on each of the
  cross-validation folds

* (Case 2) Testing the above setting in a parallel setup and monitor the difference using
  runtimes retrieved

* (Case 3) Comparing RandomSearchCV and GridSearchCV on the above task based on runtimes

* (Case 4) Running models that don't run in parallel or models which scikit-learn doesn't
  parallelize

* (Case 5) Running models that do not release the Python Global Interpreter Lock (GIL)

.. GENERATED FROM PYTHON SOURCE LINES 47-62

.. code-block:: default


    # License: BSD 3-Clause

    import openml
    import numpy as np
    from matplotlib import pyplot as plt
    from joblib.parallel import parallel_backend

    from sklearn.naive_bayes import GaussianNB
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV









.. GENERATED FROM PYTHON SOURCE LINES 63-65

Preparing tasks and scikit-learn models
***************************************

.. GENERATED FROM PYTHON SOURCE LINES 65-90

.. code-block:: default


    task_id = 167119

    task = openml.tasks.get_task(task_id)
    print(task)

    # Viewing associated data
    n_repeats, n_folds, n_samples = task.get_split_dimensions()
    print(
        "Task {}: number of repeats: {}, number of folds: {}, number of samples {}.".format(
            task_id, n_repeats, n_folds, n_samples,
        )
    )

    # Creating utility function
    def print_compare_runtimes(measures):
        for repeat, val1 in measures["usercpu_time_millis_training"].items():
            for fold, val2 in val1.items():
                print(
                    "Repeat #{}-Fold #{}: CPU-{:.3f} vs Wall-{:.3f}".format(
                        repeat, fold, val2, measures["wall_clock_time_millis_training"][repeat][fold]
                    )
                )






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    OpenML Classification Task
    ==========================
    Task Type Description: https://www.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION
    Task ID..............: 167119
    Task URL.............: https://www.openml.org/t/167119
    Estimation Procedure.: crossvalidation
    Target Feature.......: class
    # of Classes.........: 3
    Cost Matrix..........: Available
    Task 167119: number of repeats: 1, number of folds: 10, number of samples 1.




.. GENERATED FROM PYTHON SOURCE LINES 91-96

Case 1: Running a Random Forest model on an OpenML task
*******************************************************
We'll run a Random Forest model and obtain an OpenML run object. We can
see the evaluations recorded per fold for the dataset and the information
available for this run.

.. GENERATED FROM PYTHON SOURCE LINES 96-118

.. code-block:: default


    clf = RandomForestClassifier(n_estimators=10)

    run1 = openml.runs.run_model_on_task(
        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False,
    )
    measures = run1.fold_evaluations

    print("The timing and performance metrics available: ")
    for key in measures.keys():
        print(key)
    print()

    print(
        "The performance metric is recorded under `predictive_accuracy` per "
        "fold and can be retrieved as: "
    )
    for repeat, val1 in measures["predictive_accuracy"].items():
        for fold, val2 in val1.items():
            print("Repeat #{}-Fold #{}: {:.4f}".format(repeat, fold, val2))
        print()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    The timing and performance metrics available: 
    usercpu_time_millis_training
    wall_clock_time_millis_training
    usercpu_time_millis_testing
    usercpu_time_millis
    wall_clock_time_millis_testing
    wall_clock_time_millis
    predictive_accuracy

    The performance metric is recorded under `predictive_accuracy` per fold and can be retrieved as: 
    Repeat #0-Fold #0: 0.7789
    Repeat #0-Fold #1: 0.7771
    Repeat #0-Fold #2: 0.7738
    Repeat #0-Fold #3: 0.7934
    Repeat #0-Fold #4: 0.7838
    Repeat #0-Fold #5: 0.7871
    Repeat #0-Fold #6: 0.7791
    Repeat #0-Fold #7: 0.7767
    Repeat #0-Fold #8: 0.7905
    Repeat #0-Fold #9: 0.7833





.. GENERATED FROM PYTHON SOURCE LINES 119-132

The remaining entries recorded in `measures` are the runtime records
related as:

usercpu_time_millis = usercpu_time_millis_training + usercpu_time_millis_testing

wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing

The timing measures recorded as `*_millis_training` contain the per
repeat-per fold timing incurred for the execution of the `.fit()` procedure
of the model. For `usercpu_time_*` the time recorded using `time.process_time()`
is converted to `milliseconds` and stored. Similarly, `time.time()` is used
to record the time entry for `wall_clock_time_*`. The `*_millis_testing` entry
follows the same procedure but for time taken for the `.predict()` procedure.

.. GENERATED FROM PYTHON SOURCE LINES 132-136

.. code-block:: default


    # Comparing the CPU and wall-clock training times of the Random Forest model
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-225.117 vs Wall-225.131
    Repeat #0-Fold #1: CPU-219.873 vs Wall-219.890
    Repeat #0-Fold #2: CPU-222.304 vs Wall-222.323
    Repeat #0-Fold #3: CPU-222.865 vs Wall-222.873
    Repeat #0-Fold #4: CPU-223.481 vs Wall-223.489
    Repeat #0-Fold #5: CPU-225.889 vs Wall-225.924
    Repeat #0-Fold #6: CPU-228.389 vs Wall-228.389
    Repeat #0-Fold #7: CPU-230.368 vs Wall-230.416
    Repeat #0-Fold #8: CPU-221.907 vs Wall-221.944
    Repeat #0-Fold #9: CPU-223.833 vs Wall-223.855




.. GENERATED FROM PYTHON SOURCE LINES 137-140

Case 2: Running Scikit-learn model on an OpenML task in parallel
****************************************************************
Redefining the model to allow parallelism with `n_jobs=2` (2 cores)

.. GENERATED FROM PYTHON SOURCE LINES 140-150

.. code-block:: default


    clf = RandomForestClassifier(n_estimators=10, n_jobs=2)

    run2 = openml.runs.run_model_on_task(
        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False
    )
    measures = run2.fold_evaluations
    # The wall-clock time recorded per fold should be lesser than Case 1 above
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-105.349 vs Wall-822.257
    Repeat #0-Fold #1: CPU-65.339 vs Wall-181.081
    Repeat #0-Fold #2: CPU-70.348 vs Wall-181.370
    Repeat #0-Fold #3: CPU-67.822 vs Wall-182.155
    Repeat #0-Fold #4: CPU-64.357 vs Wall-181.723
    Repeat #0-Fold #5: CPU-64.419 vs Wall-191.014
    Repeat #0-Fold #6: CPU-65.447 vs Wall-171.525
    Repeat #0-Fold #7: CPU-65.253 vs Wall-178.506
    Repeat #0-Fold #8: CPU-62.408 vs Wall-166.031
    Repeat #0-Fold #9: CPU-62.747 vs Wall-176.362




.. GENERATED FROM PYTHON SOURCE LINES 151-152

Running a Random Forest model on an OpenML task in parallel (all cores available):

.. GENERATED FROM PYTHON SOURCE LINES 152-165

.. code-block:: default


    # Redefining the model to use all available cores with `n_jobs=-1`
    clf = RandomForestClassifier(n_estimators=10, n_jobs=-1)

    run3 = openml.runs.run_model_on_task(
        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False
    )
    measures = run3.fold_evaluations
    # The wall-clock time recorded per fold should be lesser than the case above,
    # if more than 2 CPU cores are available. The speed-up is more pronounced for
    # larger datasets.
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-66.111 vs Wall-176.136
    Repeat #0-Fold #1: CPU-59.374 vs Wall-161.605
    Repeat #0-Fold #2: CPU-67.273 vs Wall-164.515
    Repeat #0-Fold #3: CPU-65.074 vs Wall-173.861
    Repeat #0-Fold #4: CPU-63.866 vs Wall-178.137
    Repeat #0-Fold #5: CPU-68.206 vs Wall-202.233
    Repeat #0-Fold #6: CPU-69.267 vs Wall-173.790
    Repeat #0-Fold #7: CPU-66.140 vs Wall-176.684
    Repeat #0-Fold #8: CPU-82.310 vs Wall-210.749
    Repeat #0-Fold #9: CPU-65.008 vs Wall-180.510




.. GENERATED FROM PYTHON SOURCE LINES 166-171

We can now observe that the ratio of CPU time to wallclock time is lower
than in case 1. This happens because joblib by default spawns subprocesses
for the workloads for which CPU time cannot be tracked. Therefore, interpreting
the reported CPU and wallclock time requires knowledge of the parallelization
applied at runtime.

.. GENERATED FROM PYTHON SOURCE LINES 173-177

Running the same task with a different parallel backend. Joblib provides multiple
backends: {`loky` (default), `multiprocessing`, `dask`, `threading`, `sequential`}.
The backend can be explicitly set using a joblib context manager. The behaviour of
the job distribution can change and therefore the scale of runtimes recorded too.

.. GENERATED FROM PYTHON SOURCE LINES 177-185

.. code-block:: default


    with parallel_backend(backend="multiprocessing", n_jobs=-1):
        run3_ = openml.runs.run_model_on_task(
            model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False
        )
    measures = run3_.fold_evaluations
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-304.929 vs Wall-304.347
    Repeat #0-Fold #1: CPU-297.390 vs Wall-299.196
    Repeat #0-Fold #2: CPU-267.465 vs Wall-237.386
    Repeat #0-Fold #3: CPU-267.550 vs Wall-312.157
    Repeat #0-Fold #4: CPU-262.438 vs Wall-264.282
    Repeat #0-Fold #5: CPU-262.474 vs Wall-250.177
    Repeat #0-Fold #6: CPU-271.842 vs Wall-298.846
    Repeat #0-Fold #7: CPU-300.140 vs Wall-298.946
    Repeat #0-Fold #8: CPU-285.951 vs Wall-260.005
    Repeat #0-Fold #9: CPU-297.176 vs Wall-277.152




.. GENERATED FROM PYTHON SOURCE LINES 186-193

The CPU time interpretation becomes ambiguous when jobs are distributed over an
unknown number of cores or when subprocesses are spawned for which the CPU time
cannot be tracked, as in the examples above. It is impossible for OpenML-Python
to capture the availability of the number of cores/threads, their eventual
utilisation and whether workloads are executed in subprocesses, for various
cases that can arise as demonstrated in the rest of the example. Therefore,
the final interpretation of the runtimes is left to the `user`.

.. GENERATED FROM PYTHON SOURCE LINES 195-203

Case 3: Running and benchmarking HPO algorithms with their runtimes
*******************************************************************
We shall now optimize a similar RandomForest model for the same task using
scikit-learn's HPO support by using GridSearchCV to optimize our earlier
RandomForest model's hyperparameter `n_estimators`. Scikit-learn also provides a
`refit_time_` for such HPO models, i.e., the time incurred by training
and evaluating the model on the best found parameter setting. This is
included in the `wall_clock_time_millis_training` measure recorded.

.. GENERATED FROM PYTHON SOURCE LINES 203-224

.. code-block:: default


    from sklearn.model_selection import GridSearchCV


    clf = RandomForestClassifier(n_estimators=10, n_jobs=2)

    # GridSearchCV model
    n_iter = 5
    grid_pipe = GridSearchCV(
        estimator=clf,
        param_grid={"n_estimators": np.linspace(start=1, stop=50, num=n_iter).astype(int).tolist()},
        cv=2,
        n_jobs=2,
    )

    run4 = openml.runs.run_model_on_task(
        model=grid_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2
    )
    measures = run4.fold_evaluations
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-5524.702 vs Wall-5615.479
    Repeat #0-Fold #1: CPU-5416.309 vs Wall-5502.362
    Repeat #0-Fold #2: CPU-5353.685 vs Wall-5390.247
    Repeat #0-Fold #3: CPU-5386.712 vs Wall-5388.083
    Repeat #0-Fold #4: CPU-5572.719 vs Wall-5600.731
    Repeat #0-Fold #5: CPU-5284.268 vs Wall-5312.434
    Repeat #0-Fold #6: CPU-4906.911 vs Wall-4788.668
    Repeat #0-Fold #7: CPU-4883.351 vs Wall-5059.846
    Repeat #0-Fold #8: CPU-4856.834 vs Wall-4873.712
    Repeat #0-Fold #9: CPU-5033.915 vs Wall-4781.196




.. GENERATED FROM PYTHON SOURCE LINES 225-236

Like any optimisation problem, scikit-learn's HPO estimators also generate
a sequence of configurations which are evaluated, using which the best found
configuration is tracked throughout the trace.
The OpenML run object stores these traces as OpenMLRunTrace objects accessible
using keys of the pattern (repeat, fold, iterations). Here `fold` implies the
outer-cross validation fold as obtained from the task data splits in OpenML.
GridSearchCV here performs grid search over the inner-cross validation folds as
parameterized by the `cv` parameter. Since `GridSearchCV` in this example performs a
`2-fold` cross validation, the runtime recorded per repeat-per fold in the run object
is for the entire `fit()` procedure of GridSearchCV thus subsuming the runtimes of
the 2-fold (inner) CV search performed.

.. GENERATED FROM PYTHON SOURCE LINES 236-243

.. code-block:: default


    # We earlier extracted the number of repeats and folds for this task:
    print("# repeats: {}\n# folds: {}".format(n_repeats, n_folds))

    # To extract the training runtime of the first repeat, first fold:
    print(run4.fold_evaluations["wall_clock_time_millis_training"][0][0])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    # repeats: 1
    # folds: 10
    5615.478515625




.. GENERATED FROM PYTHON SOURCE LINES 244-247

To extract the training runtime of the 1-st repeat, 4-th (outer) fold and also
to fetch the parameters and performance of the evaluations made during
the 1-st repeat, 4-th fold evaluation by the Grid Search model.

.. GENERATED FROM PYTHON SOURCE LINES 247-264

.. code-block:: default


    _repeat = 0
    _fold = 3
    print(
        "Total runtime for repeat {}'s fold {}: {:4f} ms".format(
            _repeat, _fold, run4.fold_evaluations["wall_clock_time_millis_training"][_repeat][_fold]
        )
    )
    for i in range(n_iter):
        key = (_repeat, _fold, i)
        r = run4.trace.trace_iterations[key]
        print(
            "n_estimators: {:>2} - score: {:.3f}".format(
                r.parameters["parameter_n_estimators"], r.evaluation
            )
        )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Total runtime for repeat 0's fold 3: 5388.082981 ms
    n_estimators:  1 - score: 0.753
    n_estimators: 13 - score: 0.798
    n_estimators: 25 - score: 0.802
    n_estimators: 37 - score: 0.803
    n_estimators: 50 - score: 0.804




.. GENERATED FROM PYTHON SOURCE LINES 265-277

Scikit-learn's HPO estimators also come with an argument `refit=True` as a default.
In our previous model definition it was set to True by default, which meant that the best
found hyperparameter configuration was used to refit or retrain the model without any inner
cross validation. This extra refit time measure is provided by the scikit-learn model as the
attribute `refit_time_`.
This time is included in the `wall_clock_time_millis_training` measure.

For non-HPO estimators, `wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing`.

For HPO estimators, `wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing + refit_time`.

This refit time can therefore be explicitly extracted in this manner:

.. GENERATED FROM PYTHON SOURCE LINES 277-296

.. code-block:: default



    def extract_refit_time(run, repeat, fold):
        refit_time = (
            run.fold_evaluations["wall_clock_time_millis"][repeat][fold]
            - run.fold_evaluations["wall_clock_time_millis_training"][repeat][fold]
            - run.fold_evaluations["wall_clock_time_millis_testing"][repeat][fold]
        )
        return refit_time


    for repeat in range(n_repeats):
        for fold in range(n_folds):
            print(
                "Repeat #{}-Fold #{}: {:.4f}".format(
                    repeat, fold, extract_refit_time(run4, repeat, fold)
                )
            )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: 1136.4231
    Repeat #0-Fold #1: 1049.9194
    Repeat #0-Fold #2: 1111.5572
    Repeat #0-Fold #3: 1106.7750
    Repeat #0-Fold #4: 1185.7755
    Repeat #0-Fold #5: 874.6643
    Repeat #0-Fold #6: 697.6774
    Repeat #0-Fold #7: 924.5777
    Repeat #0-Fold #8: 561.0290
    Repeat #0-Fold #9: 533.7005




.. GENERATED FROM PYTHON SOURCE LINES 297-301

Along with the GridSearchCV already used above, we demonstrate how such
optimisation traces can be retrieved by showing an application of these
traces - comparing the speed of finding the best configuration using
RandomizedSearchCV and GridSearchCV available with scikit-learn.

.. GENERATED FROM PYTHON SOURCE LINES 301-316

.. code-block:: default


    # RandomizedSearchCV model
    rs_pipe = RandomizedSearchCV(
        estimator=clf,
        param_distributions={
            "n_estimators": np.linspace(start=1, stop=50, num=15).astype(int).tolist()
        },
        cv=2,
        n_iter=n_iter,
        n_jobs=2,
    )
    run5 = openml.runs.run_model_on_task(
        model=rs_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2
    )








.. GENERATED FROM PYTHON SOURCE LINES 317-328

Since for the call to ``openml.runs.run_model_on_task`` the parameter
``n_jobs`` is set to its default ``None``, the evaluations across the OpenML folds
are not parallelized. Hence, the time recorded is agnostic to the ``n_jobs``
being set at both the HPO estimator ``GridSearchCV`` as well as the base
estimator ``RandomForestClassifier`` in this case. The OpenML extension only records the
time taken for the completion of the complete ``fit()`` call, per-repeat per-fold.

This notion can be used to extract and plot the best found performance per
fold by the HPO model and the corresponding time taken for search across
that fold. Moreover, since ``n_jobs=None`` for ``openml.runs.run_model_on_task``
the runtimes per fold can be cumulatively added to plot the trace against time.

.. GENERATED FROM PYTHON SOURCE LINES 328-372

.. code-block:: default



    def extract_trace_data(run, n_repeats, n_folds, n_iter, key=None):
        key = "wall_clock_time_millis_training" if key is None else key
        data = {"score": [], "runtime": []}
        for i_r in range(n_repeats):
            for i_f in range(n_folds):
                data["runtime"].append(run.fold_evaluations[key][i_r][i_f])
                for i_i in range(n_iter):
                    r = run.trace.trace_iterations[(i_r, i_f, i_i)]
                    if r.selected:
                        data["score"].append(r.evaluation)
                        break
        return data


    def get_incumbent_trace(trace):
        best_score = 1
        inc_trace = []
        for i, r in enumerate(trace):
            if i == 0 or (1 - r) < best_score:
                best_score = 1 - r
            inc_trace.append(best_score)
        return inc_trace


    grid_data = extract_trace_data(run4, n_repeats, n_folds, n_iter)
    rs_data = extract_trace_data(run5, n_repeats, n_folds, n_iter)

    plt.clf()
    plt.plot(
        np.cumsum(grid_data["runtime"]), get_incumbent_trace(grid_data["score"]), label="Grid Search"
    )
    plt.plot(
        np.cumsum(rs_data["runtime"]), get_incumbent_trace(rs_data["score"]), label="Random Search"
    )
    plt.xscale("log")
    plt.yscale("log")
    plt.xlabel("Wallclock time (in milliseconds)")
    plt.ylabel("1 - Accuracy")
    plt.title("Optimisation Trace Comparison")
    plt.legend()
    plt.show()




.. image-sg:: /examples/30_extended/images/sphx_glr_fetch_runtimes_tutorial_001.png
   :alt: Optimisation Trace Comparison
   :srcset: /examples/30_extended/images/sphx_glr_fetch_runtimes_tutorial_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 373-381

Case 4: Running models that scikit-learn doesn't parallelize
*************************************************************
Both scikit-learn and OpenML depend on parallelism implemented through `joblib`.
However, there can be cases where either models cannot be parallelized or don't
depend on joblib for its parallelism. 2 such cases are illustrated below.

Running a Decision Tree model that doesn't support parallelism implicitly, but
using OpenML to parallelize evaluations for the outer-cross validation folds.

.. GENERATED FROM PYTHON SOURCE LINES 381-390

.. code-block:: default


    dt = DecisionTreeClassifier()

    run6 = openml.runs.run_model_on_task(
        model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2
    )
    measures = run6.fold_evaluations
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-85.987 vs Wall-87.572
    Repeat #0-Fold #1: CPU-83.241 vs Wall-83.299
    Repeat #0-Fold #2: CPU-82.074 vs Wall-84.320
    Repeat #0-Fold #3: CPU-81.508 vs Wall-83.280
    Repeat #0-Fold #4: CPU-80.653 vs Wall-80.674
    Repeat #0-Fold #5: CPU-80.771 vs Wall-80.782
    Repeat #0-Fold #6: CPU-80.962 vs Wall-81.601
    Repeat #0-Fold #7: CPU-82.005 vs Wall-83.848
    Repeat #0-Fold #8: CPU-79.591 vs Wall-79.987
    Repeat #0-Fold #9: CPU-79.694 vs Wall-80.982




.. GENERATED FROM PYTHON SOURCE LINES 391-394

Although the decision tree does not run in parallel, it can release the
`Python GIL <https://docs.python.org/dev/glossary.html#term-global-interpreter-lock>`_.
This can result in surprising runtime measures as demonstrated below:

.. GENERATED FROM PYTHON SOURCE LINES 394-402

.. code-block:: default


    with parallel_backend("threading", n_jobs=-1):
        run7 = openml.runs.run_model_on_task(
            model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False
        )
    measures = run7.fold_evaluations
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-176.273 vs Wall-123.448
    Repeat #0-Fold #1: CPU-177.571 vs Wall-122.766
    Repeat #0-Fold #2: CPU-172.282 vs Wall-119.290
    Repeat #0-Fold #3: CPU-148.636 vs Wall-87.712
    Repeat #0-Fold #4: CPU-182.128 vs Wall-129.446
    Repeat #0-Fold #5: CPU-181.412 vs Wall-125.315
    Repeat #0-Fold #6: CPU-168.603 vs Wall-116.860
    Repeat #0-Fold #7: CPU-166.367 vs Wall-104.728
    Repeat #0-Fold #8: CPU-172.390 vs Wall-121.205
    Repeat #0-Fold #9: CPU-136.680 vs Wall-86.892




.. GENERATED FROM PYTHON SOURCE LINES 403-406

Running a Neural Network from scikit-learn that uses scikit-learn independent
parallelism using libraries such as `MKL, OpenBLAS or BLIS
<https://scikit-learn.org/stable/computing/parallelism.html#parallel-numpy-routines-from-numerical-libraries>`_.

.. GENERATED FROM PYTHON SOURCE LINES 406-415

.. code-block:: default


    mlp = MLPClassifier(max_iter=10)

    run8 = openml.runs.run_model_on_task(
        model=mlp, task=task, upload_flow=False, avoid_duplicate_runs=False
    )
    measures = run8.fold_evaluations
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.
      warnings.warn(
    Repeat #0-Fold #0: CPU-1917.636 vs Wall-1917.763
    Repeat #0-Fold #1: CPU-2039.621 vs Wall-1931.492
    Repeat #0-Fold #2: CPU-2019.738 vs Wall-1910.181
    Repeat #0-Fold #3: CPU-2009.072 vs Wall-1899.686
    Repeat #0-Fold #4: CPU-1995.202 vs Wall-1886.154
    Repeat #0-Fold #5: CPU-2009.787 vs Wall-1902.692
    Repeat #0-Fold #6: CPU-2002.536 vs Wall-1901.633
    Repeat #0-Fold #7: CPU-2007.188 vs Wall-1898.027
    Repeat #0-Fold #8: CPU-2002.835 vs Wall-1893.784
    Repeat #0-Fold #9: CPU-2011.672 vs Wall-1901.952




.. GENERATED FROM PYTHON SOURCE LINES 416-423

Case 5: Running Scikit-learn models that don't release GIL
**********************************************************
Certain Scikit-learn models do not release the `Python GIL
<https://docs.python.org/dev/glossary.html#term-global-interpreter-lock>`_ and
are also not executed in parallel via a BLAS library. In such cases, the
CPU times and wallclock times are most likely trustworthy. Note however
that only very few models such as naive Bayes models are of this kind.

.. GENERATED FROM PYTHON SOURCE LINES 423-433

.. code-block:: default


    clf = GaussianNB()

    with parallel_backend("multiprocessing", n_jobs=-1):
        run9 = openml.runs.run_model_on_task(
            model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False
        )
    measures = run9.fold_evaluations
    print_compare_runtimes(measures)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Repeat #0-Fold #0: CPU-51.383 vs Wall-51.413
    Repeat #0-Fold #1: CPU-50.215 vs Wall-52.647
    Repeat #0-Fold #2: CPU-45.427 vs Wall-47.338
    Repeat #0-Fold #3: CPU-45.995 vs Wall-46.179
    Repeat #0-Fold #4: CPU-43.243 vs Wall-43.252
    Repeat #0-Fold #5: CPU-44.468 vs Wall-44.500
    Repeat #0-Fold #6: CPU-43.825 vs Wall-44.605
    Repeat #0-Fold #7: CPU-44.364 vs Wall-44.365
    Repeat #0-Fold #8: CPU-43.686 vs Wall-43.696
    Repeat #0-Fold #9: CPU-43.461 vs Wall-43.493




.. GENERATED FROM PYTHON SOURCE LINES 434-480

Summmary
*********
The scikit-learn extension for OpenML-Python records model runtimes for the
CPU-clock and the wall-clock times. The above examples illustrated how these
recorded runtimes can be extracted when using a scikit-learn model and under
parallel setups too. To summarize, the scikit-learn extension measures the:

* `CPU-time` & `wallclock-time` for the whole run

  * A run here corresponds to a call to `run_model_on_task` or `run_flow_on_task`
  * The recorded time is for the model fit for each of the outer-cross validations folds,
    i.e., the OpenML data splits

* Python's `time` module is used to compute the runtimes

  * `CPU-time` is recorded using the responses of `time.process_time()`
  * `wallclock-time` is recorded using the responses of `time.time()`

* The timings recorded by OpenML per outer-cross validation fold is agnostic to
  model parallelisation

  * The wallclock times reported in Case 2 above highlights the speed-up on using `n_jobs=-1`
    in comparison to `n_jobs=2`, since the timing recorded by OpenML is for the entire
    `fit()` procedure, whereas the parallelisation is performed inside `fit()` by scikit-learn
  * The CPU-time for models that are run in parallel can be difficult to interpret

* `CPU-time` & `wallclock-time` for each search per outer fold in an HPO run

  * Reports the total time for performing search on each of the OpenML data split, subsuming
    any sort of parallelism that happened as part of the HPO estimator or the underlying
    base estimator
  * Also allows extraction of the `refit_time` that scikit-learn measures using `time.time()`
    for retraining the model per outer fold, for the best found configuration

* `CPU-time` & `wallclock-time` for models that scikit-learn doesn't parallelize

  * Models like Decision Trees or naive Bayes don't parallelize and thus both the wallclock and
    CPU times are similar in runtime for the OpenML call
  * However, models implemented in Cython, such as the Decision Trees can release the GIL and
    still run in parallel if a `threading` backend is used by joblib.
  * Scikit-learn Neural Networks can undergo parallelization implicitly owing to thread-level
    parallelism involved in the linear algebraic operations and thus the wallclock-time and
    CPU-time can differ.

Because of all the cases mentioned above it is crucial to understand which case is triggered
when reporting runtimes for scikit-learn models measured with OpenML-Python!


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  18.946 seconds)


.. _sphx_glr_download_examples_30_extended_fetch_runtimes_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: fetch_runtimes_tutorial.py <fetch_runtimes_tutorial.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: fetch_runtimes_tutorial.ipynb <fetch_runtimes_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
